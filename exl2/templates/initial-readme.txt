# {AUTHOR}/{MODEL} Exllama2 Quantized

This is an Exllama2 quantized version of the [{MODEL}](https://huggingface.co/{AUTHOR}/{MODEL}) model.

## Model Details

- **Original model**: [{AUTHOR}/{MODEL}](https://huggingface.co/{AUTHOR}/{MODEL})
- **Quantization method**: Exllama2
- **Bits per weight**: {BPW}
- **Head bits**: {HEAD_BITS}

## Usage

This model is compatible with the Exllama2 library. For usage instructions, please refer to the [Exllama2 documentation](https://github.com/turboderp/exllamav2).

## Quantization Process

This model was quantized using the SolidRusT Networks quantization pipeline. The quantization process preserves the model's performance while reducing its size and memory footprint.

## License

This model inherits the license of the original model. Please refer to the original model's license for usage terms and conditions.